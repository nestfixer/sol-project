# Cline's Project Intelligence

This file captures important patterns, preferences, and project intelligence that will help me work more effectively with the Solana Token Analysis Agent Swarm project.

## Solana Agent Kit Integration Notes

### MCP Server Development Patterns
1. **Node.js Best Practices**
   - Use ES Modules for imports/exports (required for MCP SDK)
   - Implement proper error handling in promise chains
   - Use clean async/await patterns throughout
   - Leverage file-based persistence for storing state

2. **MCP Server Structure**
   - Each server should have a clear, focused purpose
   - Tools should have descriptive names and intuitive parameters
   - Error messages should be detailed and actionable
   - All operations should be properly logged for debugging

3. **Cross-Language Integration**
   - Use JSON for all data exchange between Python and Node.js
   - Implement consistent error handling patterns
   - Define clear data schemas for all exchanged objects
   - Create thorough documentation for all integration points

### Solana Agent Kit Usage
1. **Environment Setup**
   - Store private keys as environment variables, never in code
   - Use different RPC endpoints for development vs. production
   - Configure appropriate rate limiting for API calls
   - Test API connectivity before performing blockchain operations

2. **Action Execution**
   - Simulate transactions before executing when possible
   - Implement proper error handling for network failures
   - Add retry logic for transient blockchain errors
   - Log all transaction details for audit purposes

3. **Tool Integration**
   - Use `createSolanaTools` for LangChain integration
   - Access actions directly via `SolanaAgentKit` instance
   - Leverage TypeScript interfaces for proper typing
   - Follow defined patterns for asynchronous operations

## Phase 2 Implementation Notes

### Solana RPC Integration Best Practices
1. **Connection Management**
   - Implement connection pooling for RPC endpoints
   - Use exponential backoff for retry logic
   - Cache common blockchain queries
   - Maintain WebSocket connections with heartbeats

2. **Token Data Processing**
   - Normalize token data to standard schema before storage
   - Extract key metrics: supply, holders, price, volume
   - Calculate derived metrics: holder concentration, liquidity ratio
   - Apply consistent timestamp formatting: ISO 8601 UTC

3. **WebSocket Subscription Handling**
   - Maintain subscription state between reconnections
   - Buffer events during processing backpressure
   - Use connection keep-alive mechanisms
   - Implement graceful degradation to polling when websocket fails

### Inter-Agent Communication Patterns
1. **Message Flow**
   - Orchestrator acts as central message router (not a bus)
   - Use direct point-to-point messaging for synchronous operations
   - Use broadcast patterns for token discovery events
   - Include source and target agent IDs for all directed messages

2. **Request-Response Cycle**
   - Always use correlation IDs to match responses to requests
   - Include timeouts for all request-response patterns
   - Structure response messages with standard success/error fields
   - Handle partial responses for long-running operations

3. **Event Broadcasting**
   - Use the EVENT message type for notifications
   - Include detailed event metadata (timestamp, source, event type)
   - Structure event payloads consistently
   - Allow agents to subscribe to specific event types

### Knowledge Base Usage Patterns
1. **Data Organization**
   - Use consistent prefix conventions for entry IDs (e.g., "token:", "pattern:")
   - Group related data with common tags
   - Include source agent information with all entries
   - Set appropriate TTL for different data types

2. **Query Optimization**
   - Use specific entry types when querying to improve performance
   - Filter by tags for focused results
   - Include custom filter functions for complex queries
   - Cache repeated query results when appropriate

## Code Patterns and Conventions

### Python Best Practices

1. **Type Annotations**
   - All function parameters and return values should use type annotations
   - Use `Optional[Type]` for parameters that can be None
   - Use `List`, `Dict`, `Set` from `typing` module for container types

2. **Async/Await Usage**
   - All agent methods use `async/await` patterns
   - Message handling is asynchronous
   - Use `asyncio.Queue` for message passing
   - Avoid blocking operations in async functions

3. **Error Handling**
   - Use structured try/except blocks
   - Log errors with appropriate context
   - Update agent status to reflect error conditions
   - Propagate errors through message system when appropriate

4. **Confidence Level Reporting**
   - Use the `send_confidence_report` method after completing significant tasks
   - Always provide a confidence score on the 1-10 scale (10 being highest confidence)
   - Include task name for clear identification and tracking
   - Add detailed context in the optional details parameter when relevant
   - The method handles validation and logging automatically

### Project-Specific Conventions

1. **Agent Implementation**
   - All agents inherit from the base `Agent` class
   - Override the abstract methods: `_initialize()`, `_handle_message()`, `_cleanup()`
   - Register with the orchestrator during initialization
   - Update status periodically

2. **Message Handling**
   - Messages should follow the `Message` class structure
   - Use appropriate `MessageType` enum values
   - Include correlation IDs for request/response tracking
   - Set appropriate priority levels (1-10)
   - Include confidence levels for analysis results (1-10)

3. **Knowledge Base Interaction**
   - All token data should be stored in the knowledge base
   - Use consistent key structures for token lookup
   - Include timestamp information with all stored data
   - Query with specific criteria rather than retrieving large datasets

## Implementation Notes

1. **Agent Lifecycle**
   - Initialization → Ready → Running → Stopped flow
   - Handle transitions between states gracefully
   - Clean up resources during stop operations
   - Report status changes to orchestrator
   - Report confidence levels after completing tasks

2. **Message Processing**
   - Messages processed in priority order
   - Inbound messages trigger handler callbacks
   - Outbound messages routed through orchestrator
   - Use message correlation to track request chains

3. **Blockchain Integration**
   - Handle network timeouts and retries
   - Normalize data from RPC responses
   - Use WebSocket for real-time events when possible
   - Fallback to polling when WebSocket not available

## System Gotchas and Insights

1. **Common Pitfalls**
   - Avoid blocking operations in async message handlers
   - Watch for memory growth in the knowledge base
   - Handle WebSocket disconnections gracefully
   - Ensure message queues don't grow unbounded
   - Always include confidence levels when reporting analysis results

2. **Performance Considerations**
   - Batch blockchain RPC requests when possible
   - Implement caching for frequently accessed data
   - Use lightweight message structures
   - Monitor queue sizes for backpressure detection

3. **Extension Points**
   - New agent types can be added by subclassing `Agent`
   - Analysis algorithms can be plugged into existing agents
   - Configuration system allows runtime parameter adjustment
   - Message types can be extended for special purposes

## Debugging Approaches

1. **Logging Strategies**
   - Use appropriate log levels (debug, info, warning, error)
   - Include context information in log messages
   - Log message IDs for tracing request flows
   - Enable debug logging selectively for specific components
   - Check confidence level logs to identify low-confidence analyses

2. **Confidence Level Guidelines**
   - 9-10: High confidence based on strong patterns and comprehensive data
   - 7-8: Good confidence with clear signals but some uncertainties
   - 5-6: Moderate confidence with mixed signals
   - 3-4: Low confidence with significant uncertainties
   - 1-2: Very low confidence, highly speculative or based on minimal data

3. **Common Issues**
   - Agents not receiving messages: Check registration with orchestrator
   - System appears to hang: Look for blocked async operations
   - High error rates: Check Solana RPC connectivity
   - Inconsistent results: Verify knowledge base queries

4. **Development Tools**
   - Use CLI's debug commands for agent inspection
   - Monitor agent status metrics for health checks
   - Examine message queues for processing bottlenecks
   - Check task registry for stuck operations

## Project Roadmap Implementation Guidelines

### Additional Blockchain Data Collection

1. **Smart Contract Analysis Implementation**
   - Use decompilers for BPF programs (Solana's bytecode)
   - Scan for known vulnerability patterns
   - Compare against reference implementation patterns
   - Track authority changes and upgradability status

2. **Enhanced Token Metrics Collection**
   - Query multiple DEXs for comprehensive liquidity data
   - Establish baselines for holder distribution to detect anomalies
   - Tag known team/treasury wallets for special monitoring
   - Monitor cross-chain bridges for wrapped token tracking

### New Tools & Libraries Integration

1. **Machine Learning Framework Best Practices**
   - Use TensorFlow.js for browser-compatible models
   - Implement scikit-learn for statistical analysis in Python components
   - Consider PyTorch for advanced pattern recognition
   - Separate model training from inference pipelines

2. **Solana Ecosystem Integration Patterns**
   - Use Helius API for enriched transaction data
   - Leverage Jupiter for DEX aggregation and price discovery
   - Implement Switchboard for reliable price feeds
   - Use dedicated SPL interfaces for token standard compliance

3. **Visualization Tool Integration**
   - Implement D3.js for custom, interactive visualizations
   - Use ECharts for financial pattern visualization
   - Consider TradingView for familiar trading interfaces
   - Keep visualization code separate from data processing

### Testing & Iteration Strategy

1. **Simulated Environment Setup**
   - Create historical data replay capability with consistent format
   - Build scenario generator for edge cases
   - Implement corpus of known patterns with labels
   - Support time-compression for accelerated testing

2. **Testnet Deployment Guidelines**
   - Use separate configuration for testnet vs. mainnet
   - Create test tokens with specific characteristics
   - Document test scenarios in a structured format
   - Implement automated verification of expected outcomes

3. **Metrics Collection Framework**
   - Define key performance indicators for each component
   - Implement structured logging for metric extraction
   - Create dashboards for real-time monitoring
   - Establish baselines for comparison

4. **A/B Testing Methodology**
   - Run parallel algorithm versions with shared input
   - Compare outputs using predefined evaluation criteria
   - Implement statistical significance testing
   - Document findings and justifications for algorithm selection

### Production Architecture Guidelines

1. **Horizontal Scaling Implementation**
   - Design stateless components where possible
   - Implement work distribution mechanisms
   - Use consistent hashing for deterministic routing
   - Consider containerization for deployment flexibility

2. **Database Persistence Strategy**
   - Use time-series database for historical data
   - Implement document store for token metadata
   - Consider key-value store for high-throughput data
   - Create clear data retention and pruning policies

3. **Cross-Language Communication Standards**
   - Define schema using Protocol Buffers or similar
   - Implement versioning for all message types
   - Create comprehensive error mapping between languages
   - Build validation on both sides of language boundary

4. **Caching Implementation**
   - Use tiered caching strategy (memory → disk → network)
   - Implement cache invalidation based on data type
   - Consider distributed caching for clustered deployment
   - Monitor cache hit rates and optimize accordingly
